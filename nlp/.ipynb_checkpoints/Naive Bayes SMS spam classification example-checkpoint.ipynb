{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes SMS spam classification example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "smsdata = open('../dataset/smsspamcollection/SMSSpamCollection.txt', 'r')\n",
    "csv_reader = csv.reader(smsdata, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal coding starts  from here as usual\n",
    "smsdata_data = []\n",
    "smsdata_labels = []\n",
    "\n",
    "for line in csv_reader:\n",
    "    smsdata_labels.append(line[0])\n",
    "    smsdata_data.append(line[1])\n",
    "\n",
    "smsdata.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smsdata_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat... ham\n",
      "Ok lar... Joking wif u oni... ham\n",
      "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's spam\n",
      "U dun say so early hor... U c already then say... ham\n",
      "Nah I don't think he goes to usf, he lives around here though ham\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(smsdata_data[i], smsdata_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'ham': 4825, 'spam': 747})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(smsdata_labels)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    \n",
    "    # The following line of the code splits the word and checks each character if it is in standard\n",
    "    # punctuations, if so it will be replaced with blank and or else it just does not replace with\n",
    "    # blanks:\n",
    "    text2 = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in text]).split())\n",
    "    \n",
    "    # The following code tokenizes the sentences into words based on white spaces and put them\n",
    "    # together as a list for applying further steps:\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text2) for word in nltk.word_tokenize(sent)]\n",
    "    \n",
    "    # Converting all the cases (upper, lower, and proper) into lowercase reduces duplicates in\n",
    "    # corpus:\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    \n",
    "    # As mentioned earlier, stop words are the words that do not carry much weight in\n",
    "    # understanding the sentence; they are used for connecting words, and so on. We have\n",
    "    # removed them with the following line of code:\n",
    "    stopwds = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stopwds]\n",
    "    \n",
    "    # Keeping only the words with length greater than 3 in the following code for removing small\n",
    "    # words, which hardly consists of much of a meaning to carry:\n",
    "    tokens = [word for word in tokens if len(word) >= 3]\n",
    "    \n",
    "    # Stemming is applied on the words using PorterStemmer function, which stems the extra\n",
    "    # suffixes from the words:\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # POS tagging is a prerequisite for lemmatization, based on whether the word is noun or\n",
    "    # verb, and so on, it will reduce it to the root word:\n",
    "    tagged_corpus = pos_tag(tokens)\n",
    "    \n",
    "    # The pos_tag function returns the part of speed in four formats for noun and six formats for\n",
    "    # verb. NN (noun, common, singular), NNP (noun, proper, singular), NNPS (noun, proper,\n",
    "    # plural), NNS (noun, common, plural), VB (verb, base form), VBD (verb, past tense), VBG (verb,\n",
    "    # present participle), VBN (verb, past participle), VBP (verb, present tense, not third person\n",
    "    # singular), VBZ (verb, present tense, third person singular):\n",
    "    Noun_tags = ['NN', 'NNP', 'NNPS', 'NNS']\n",
    "    Verb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # The prat_lemmatize function has been created only for the reasons of mismatch between\n",
    "    # the pos_tag function and intake values of the lemmatize function. If the tag for any word\n",
    "    # falls under the respective noun or verb tags category, n or v will be applied accordingly in\n",
    "    # the lemmatize function:\n",
    "    \n",
    "    def prat_lemmatize(token, tag):\n",
    "        if tag in Noun_tags:\n",
    "            return lemmatizer.lemmatize(token, 'n')\n",
    "        elif tag in Verb_tags:\n",
    "            return lemmatizer.lemmatize(token, 'v')\n",
    "        else:\n",
    "            return lemmatizer.lemmatize(token, 'n')\n",
    "    \n",
    "    # After performing tokenization and applied all the various operations, we need to join it\n",
    "    # back to form stings and the following function performs the same:\n",
    "    pre_proc_text = \" \".join([prat_lemmatize(token, tag) for token, tag in tagged_corpus])\n",
    "    return pre_proc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "smsdata_data_2 = []\n",
    "for i in smsdata_data:\n",
    "    smsdata_data_2.append(preprocessing(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set size of this classifier is 3900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "trainset_size = int(round(len(smsdata_data_2)*0.70))\n",
    "print('The training set size of this classifier is ' + str(trainset_size) + '\\n')\n",
    "X_train = np.array([''.join(rec) for rec in smsdata_data_2[0:trainset_size]])\n",
    "y_train = np.array([rec for rec in smsdata_labels[0:trainset_size]])\n",
    "\n",
    "X_test = np.array([''.join(rec) for rec in smsdata_data_2[trainset_size + 1:len(smsdata_data_2)]])\n",
    "y_test = np.array([rec for rec in smsdata_labels[trainset_size + 1:len(smsdata_labels)]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code converts the words into a vectorizer format and applies term\n",
    "frequency-inverse document frequency (TF-IDF) weights, which is a way to increase\n",
    "weights to words with high frequency and at the same time penalize the general terms such\n",
    "as the, him, at, and so on. In the following code, we have restricted to most frequent 4,000\n",
    "words in the vocabulary, none the less we can tune this parameter as well for checking\n",
    "where the better accuracies are obtained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building TFIDF vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1, 2), stop_words='english', max_features=4000,\n",
    "                           strip_accents='unicode', norm='l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TF-IDF transformation has been shown as follows on both train and test data. The\n",
    "todense function is used to create the data to visualize the content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2 = vectorizer.fit_transform(X_train).todense()\n",
    "X_test_2 = vectorizer.transform(X_test).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Naive Bayes classifier is suitable for classification with discrete features\n",
    "(example word counts), which normally requires large feature counts. However, in practice,\n",
    "fractional counts such as TF-IDF will also work well. If we do not mention any Laplace\n",
    "estimator, it does take the value of 1.0 means and it will add 1.0 against each term in\n",
    "numerator and total for denominator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes - Train Confusion Matrix\n",
      "\n",
      "Predicted   ham  spam\n",
      "Actuall              \n",
      "ham        3381     0\n",
      "spam         77   442\n",
      "\n",
      "Naive Bayes - Train accuracy 0.98\n",
      "\n",
      "Naive Bayes - Train Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.98      1.00      0.99      3381\n",
      "       spam       1.00      0.85      0.92       519\n",
      "\n",
      "avg / total       0.98      0.98      0.98      3900\n",
      "\n",
      "\n",
      "Naive Bayes - Test Confusion Matrix\n",
      "\n",
      "Predicted   ham  spam\n",
      "Actuall              \n",
      "ham        1440     3\n",
      "spam         54   174\n",
      "\n",
      "Naive Bayes - Test accuracy 0.966\n",
      "\n",
      "Naive Bayes - Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.96      1.00      0.98      1443\n",
      "       spam       0.98      0.76      0.86       228\n",
      "\n",
      "avg / total       0.97      0.97      0.96      1671\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_2, y_train)\n",
    "\n",
    "ytrain_nb_predicted = clf.predict(X_train_2)\n",
    "ytest_nb_predicted = clf.predict(X_test_2)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "print('\\nNaive Bayes - Train Confusion Matrix\\n\\n', pd.crosstab(y_train, ytrain_nb_predicted,\n",
    "                                                               rownames=['Actuall'], colnames=['Predicted']), sep='')\n",
    "print('\\nNaive Bayes - Train accuracy', round(accuracy_score(y_train, ytrain_nb_predicted), 3))\n",
    "print('\\nNaive Bayes - Train Classification Report\\n', classification_report(y_train, ytrain_nb_predicted))\n",
    "\n",
    "\n",
    "print('\\nNaive Bayes - Test Confusion Matrix\\n\\n', pd.crosstab(y_test, ytest_nb_predicted,\n",
    "                                                               rownames=['Actuall'], colnames=['Predicted']), sep='')\n",
    "print('\\nNaive Bayes - Test accuracy', round(accuracy_score(y_test, ytest_nb_predicted), 3))\n",
    "print('\\nNaive Bayes - Test Classification Report\\n', classification_report(y_test, ytest_nb_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous results it is appearing that Naive Bayes has produced excellent results of\n",
    "96.6 percent test accuracy with significant recall value of 76 percent for spam and almost\n",
    "100 percent for ham.\n",
    "However, if we would like to check what are the top 10 features based on their coefficients\n",
    "from Naive Bayes, the following code will be handy for this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we would like to check what are the top 10 features based on their coefficients\n",
    "from Naive Bayes, the following code will be handy for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 features - both first & last\n",
      "\n",
      "\t-8.7130\t1hr            \t\t-5.5795\tfree           \n",
      "\t-8.7130\t1st love       \t\t-5.7187\ttxt            \n",
      "\t-8.7130\t2go            \t\t-5.8721\ttext           \n",
      "\t-8.7130\t2morrow        \t\t-6.0066\tclaim          \n",
      "\t-8.7130\t2mrw           \t\t-6.0704\tstop           \n",
      "\t-8.7130\t2nd inning     \t\t-6.0785\tmobil          \n",
      "\t-8.7130\t2nd sm         \t\t-6.1074\trepli          \n",
      "\t-8.7130\t30ish          \t\t-6.1514\tprize          \n",
      "\t-8.7130\t3rd            \t\t-6.2015\tservic         \n",
      "\t-8.7130\t3rd natur      \t\t-6.2208\ttone           \n"
     ]
    }
   ],
   "source": [
    "# printing top features\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "coefs = clf.coef_\n",
    "intercept = clf.intercept_\n",
    "coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "\n",
    "print('\\nTop 10 features - both first & last\\n')\n",
    "n=10\n",
    "top_n_coefs = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "for (coef_1, fn_1), (coef_2, fn_2) in top_n_coefs:\n",
    "    print('\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s' % (coef_1, fn_1, coef_2, fn_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-5.579497917408297, 'free'),\n",
       " (-5.718713033530435, 'txt'),\n",
       " (-5.872109068378337, 'text'),\n",
       " (-6.006611788084338, 'claim'),\n",
       " (-6.070368904423681, 'stop'),\n",
       " (-6.078481599936406, 'mobil'),\n",
       " (-6.107370536961926, 'repli'),\n",
       " (-6.151415272013495, 'prize'),\n",
       " (-6.201462737225315, 'servic'),\n",
       " (-6.2207812681156, 'tone')]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seq[start:end:step]\n",
    "coefs_with_fns[:-11:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.01682795])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
